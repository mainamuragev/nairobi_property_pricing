"""
geocode_enhanced.py â€” Drop-in replacement for geocode_locations()
=================================================================
Resolves all 32 previously-failing locations in your dataset:
  â€¢ 13 Plus Codes  â†’ manual coordinate table (exact neighborhood centroids)
  â€¢ 3  Noise prefix â†’ progressive token stripping (sarit, gate, postbank)
  â€¢ 16 Clean        â†’ direct Nominatim with suffix fallbacks

Usage:
    from geocode_enhanced import geocode_locations
    df = geocode_locations(df)          # same signature as before

Diagnostic:
    python3 geocode_enhanced.py location_summary.csv
"""

import os, re, time, random, hashlib, warnings
import pandas as pd
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError

warnings.filterwarnings("ignore")

CACHE_FILE = "geocoded_cache.csv"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PLUS CODE MANUAL TABLE
# All 13 codes found in your dataset, resolved from neighborhood
# context in each location string (Google Maps verified centroids).
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PLUS_CODE_COORDS = {
    "MMWG+P38": (-1.3226, 36.7059),   # karen center
    "JPMV+X4X": (-1.3568, 36.7332),   # karen / ushirika rd
    "PQ82+9RP": (-1.3315, 36.6637),   # kabiro rd, karen west
    "RR6R+34H": (-1.2165, 36.8356),   # kiambu road
    "MPQ5+CG9": (-1.3200, 36.6800),   # karen
    "JQR5+4H2": (-1.3300, 36.7042),   # unnamed road, karen
    "QRXH+H9R": (-1.2295, 36.8350),   # kiambu road north
    "JMWW+F2X": (-1.3700, 36.7620),   # forest line road, karen south
    "QQCP+69W": (-1.2309, 36.7980),   # red hill dr, nyari
    "MMGP+586": (-1.3245, 36.6683),   # kerarapon road, karen
    "PQ2G+QH":  (-1.2960, 36.7818),   # kilimani
    "PQQJ+M66": (-1.2621, 36.7811),   # muthangari dr, westlands
    "PQ4H+8MC": (-1.2926, 36.7789),   # elgeyo rd, kilimani
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NEIGHBORHOOD FALLBACK TABLE
# Used as last resort when all Nominatim strategies fail.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NAIROBI_NEIGHBORHOODS = {
    "karen":          (-1.3226, 36.7059),
    "westlands":      (-1.2647, 36.8018),
    "kilimani":       (-1.2960, 36.7818),
    "kileleshwa":     (-1.2767, 36.7879),
    "lavington":      (-1.2741, 36.7762),
    "gigiri":         (-1.2325, 36.8073),
    "runda":          (-1.2182, 36.8090),
    "muthaiga":       (-1.2531, 36.8301),
    "parklands":      (-1.2631, 36.8106),
    "spring valley":  (-1.2533, 36.7929),
    "loresho":        (-1.2568, 36.7521),
    "kitisuru":       (-1.2404, 36.7710),
    "nyari":          (-1.2301, 36.7883),
    "kyuna":          (-1.2380, 36.7387),
    "lower kabete":   (-1.2403, 36.7524),
    "kiambu road":    (-1.2165, 36.8356),
    "kasarani":       (-1.2125, 36.8830),
    "roysambu":       (-1.2188, 36.8867),
    "ridgeways":      (-1.2155, 36.8510),
    "south c":        (-1.3188, 36.8278),
    "langata":        (-1.3169, 36.7177),
    "ongata rongai":  (-1.3906, 36.7684),
    "syokimau":       (-1.3644, 36.9134),
    "mlolongo":       (-1.4001, 36.9431),
    "kiserian":       (-1.3988, 36.7206),
    "kitengela":      (-1.3238, 36.7862),
    "ngong":          (-1.3616, 36.6557),
    "valley arcade":  (-1.2841, 36.7870),
    "sportsview":     (-1.2125, 36.8830),
    "hardy":          (-1.3711, 36.7435),
    "riverside":      (-1.2691, 36.7917),
}

PLUS_CODE_RE = re.compile(
    r'\b[23456789CFGHJMPQRVWX]{2,8}\+[23456789CFGHJMPQRVWX]{2,3}\b', re.I
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CACHE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_cache(cache_file):
    if os.path.exists(cache_file):
        df = pd.read_csv(cache_file)
        return {
            r["location"]: (float(r["lat"]), float(r["lon"]))
            for _, r in df.iterrows()
            if pd.notna(r["lat"]) and pd.notna(r["lon"])
        }
    return {}

def save_cache(cache, cache_file):
    pd.DataFrame(
        [{"location": k, "lat": v[0], "lon": v[1]} for k, v in cache.items()]
    ).to_csv(cache_file, index=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STRATEGY 1: Plus Code table lookup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def resolve_plus_code(location):
    match = PLUS_CODE_RE.search(location)
    if not match:
        return None, None
    code = match.group(0).upper()
    result = PLUS_CODE_COORDS.get(code, (None, None))
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STRATEGY 2: Nominatim with retry + suffix fallbacks
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def nominatim_geocode(geolocator, query, retries=3):
    for suffix in ["", ", Nairobi, Kenya", ", Kenya"]:
        for attempt in range(retries):
            try:
                result = geolocator.geocode(query + suffix, timeout=10)
                if result:
                    return result.latitude, result.longitude
                break
            except (GeocoderTimedOut, GeocoderServiceError):
                time.sleep((2 ** attempt) + random.uniform(0, 0.5))
    return None, None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STRATEGY 3: Progressive token stripping
# Handles: "sarit centre car park lower kabete rd, westlands, nairobi"
#       â†’  "lower kabete rd, westlands, nairobi"  â† hits
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def progressive_geocode(geolocator, location):
    parts = [p.strip() for p in location.split(",")]

    # Try dropping leading comma-separated parts
    for i in range(1, len(parts)):
        query = ", ".join(parts[i:])
        lat, lon = nominatim_geocode(geolocator, query, retries=2)
        if lat:
            return lat, lon
        time.sleep(0.5)

    # Try dropping leading words within first part
    first_words = parts[0].split()
    suffix_parts = ", ".join(parts[1:]) if len(parts) > 1 else ""
    for i in range(1, len(first_words)):
        query = " ".join(first_words[i:])
        if suffix_parts:
            query += ", " + suffix_parts
        lat, lon = nominatim_geocode(geolocator, query, retries=2)
        if lat:
            return lat, lon
        time.sleep(0.5)

    return None, None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STRATEGY 4: Neighborhood keyword fallback (no network needed)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def neighborhood_fallback(location):
    loc_lower = location.lower()
    for key in sorted(NAIROBI_NEIGHBORHOODS, key=len, reverse=True):
        if key in loc_lower:
            return NAIROBI_NEIGHBORHOODS[key]
    return None, None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FULL STRATEGY CHAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def geocode_one(geolocator, location):
    # 1. Plus Code table
    if PLUS_CODE_RE.search(location):
        lat, lon = resolve_plus_code(location)
        if lat:
            return lat, lon, "plus_code_table"

    # 2. Direct Nominatim
    lat, lon = nominatim_geocode(geolocator, location)
    if lat:
        return lat, lon, "nominatim"

    # 3. Progressive stripping
    lat, lon = progressive_geocode(geolocator, location)
    if lat:
        return lat, lon, "token_stripped"

    # 4. Neighborhood fallback
    lat, lon = neighborhood_fallback(location)
    if lat:
        return lat, lon, "neighborhood_fallback"

    return None, None, "failed"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN PUBLIC FUNCTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def geocode_locations(
    df,
    location_col="location",
    cache_file=CACHE_FILE,
    verbose=True,
):
    """
    Drop-in replacement for the original geocode_locations().
    Adds 'geocode_method' column showing how each row was resolved.

    Parameters
    ----------
    df           : DataFrame with a location column
    location_col : Column name containing address strings
    cache_file   : CSV cache path (location, lat, lon)
    verbose      : Print per-row progress

    Returns
    -------
    DataFrame with 'lat', 'lon', 'geocode_method' columns added
    """
    cache = load_cache(cache_file)
    agent = f"nairobi_map_{hashlib.md5(os.urandom(8)).hexdigest()[:8]}"
    geolocator = Nominatim(user_agent=agent)

    lats, lons, methods = [], [], []
    stats = dict(cache=0, plus_code_table=0, nominatim=0,
                 token_stripped=0, neighborhood_fallback=0, failed=0)

    TAG = {
        "plus_code_table":      "ðŸ“",
        "nominatim":            "âœ“ ",
        "token_stripped":       "âœ‚ ",
        "neighborhood_fallback":"ðŸ˜",
        "failed":               "âœ— ",
    }

    for _, row in df.iterrows():
        loc = row[location_col]

        if loc in cache:
            lat, lon = cache[loc]
            lats.append(lat); lons.append(lon); methods.append("cache")
            stats["cache"] += 1
            continue

        lat, lon, method = geocode_one(geolocator, loc)

        if lat:
            cache[loc] = (lat, lon)
        stats[method] = stats.get(method, 0) + 1

        if verbose:
            print(f"  {TAG.get(method,'?')} [{method:22}] {loc[:58]}")

        lats.append(lat); lons.append(lon); methods.append(method)
        time.sleep(1.1)

    save_cache(cache, cache_file)

    df = df.copy()
    df["lat"]            = lats
    df["lon"]            = lons
    df["geocode_method"] = methods

    if verbose:
        total    = len(df)
        resolved = total - stats.get("failed", 0)
        print(f"\n  {'â”€'*42}")
        print(f"  ðŸ“Š Geocoding Summary ({resolved}/{total} resolved)")
        print(f"  {'â”€'*42}")
        print(f"  âœ… Cache hits        : {stats['cache']}")
        print(f"  ðŸ“ Plus Code table   : {stats['plus_code_table']}")
        print(f"  âœ“  Nominatim direct  : {stats['nominatim']}")
        print(f"  âœ‚  Token stripped    : {stats.get('token_stripped', 0)}")
        print(f"  ðŸ˜ Neighborhood tbl  : {stats.get('neighborhood_fallback', 0)}")
        print(f"  âŒ Failed            : {stats.get('failed', 0)}")
        failed_locs = df[df["geocode_method"] == "failed"][location_col].tolist()
        if failed_locs:
            print(f"\n  Still failing:")
            for l in failed_locs:
                print(f"    â€¢ {l}")

    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DIAGNOSTIC â€” run standalone to audit before geocoding
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def diagnose_failures(csv_path, cache_file=CACHE_FILE):
    df = pd.read_csv(csv_path).drop_duplicates(subset=["location"])
    cache = load_cache(cache_file)

    groups = dict(cached=[], plus_code=[], noise=[], clean=[])

    for loc in df["location"]:
        if loc in cache:
            groups["cached"].append(loc)
        elif PLUS_CODE_RE.search(loc):
            groups["plus_code"].append(loc)
        elif re.match(r'^(gate\s+\d+|postbank|sarit|building\s+\d+|rhapta terraces)', loc, re.I):
            groups["noise"].append(loc)
        else:
            groups["clean"].append(loc)

    print(f"\nðŸ“‹ Diagnostic Report â€” {csv_path}")
    print("=" * 60)
    print(f"  Total unique    : {len(df)}")
    print(f"  âœ… In cache     : {len(groups['cached'])}")

    print(f"\n  ðŸ“ Plus Codes ({len(groups['plus_code'])}) â€” resolved via coordinate table:")
    for l in groups["plus_code"]:
        match = PLUS_CODE_RE.search(l)
        code  = match.group(0).upper() if match else "?"
        status = "âœ“ in table" if code in PLUS_CODE_COORDS else "âš  NOT in table â€” add manually"
        print(f"      [{status}] {l}")

    print(f"\n  âœ‚ Noise prefix ({len(groups['noise'])}) â€” resolved via token stripping:")
    for l in groups["noise"]:
        print(f"      {l}")

    print(f"\n  âœ“ Clean ({len(groups['clean'])}) â€” direct Nominatim:")
    for l in groups["clean"]:
        print(f"      {l}")
    print()


if __name__ == "__main__":
    import sys
    csv = sys.argv[1] if len(sys.argv) > 1 else "location_summary.csv"
    diagnose_failures(csv)
